# Quick reference

# euler-copilot-fast-inference | openEuler
An inference framework that can be used for large-scale CPU inference, based on the Kunpeng and openEuler basic image, containerized package after release, can be used for one-click deployment

# Supported tags and respective Dockerfile links
tags of the current container image: openeuler/euler-copilot-fast-inference:1.0.0-oe2203sp3
Link to dockerfile is (https://gitee.com/openeuler/openeuler-docker-images/blob/master/euler-copilot-fast-inference/1.0.0/22.03-lts-sp3/Dockerfile)

# Usageï¼š
1. Start the container based on the application image
2. Access the /home/euler-copilot-fast-inference directory to get the inference process
3. Specify the large model path and execute the inference process
4. Specify the large model path as the first argument, using -i to specify the input prompt word
5. View model inference results

# Question and answering
If you have any questions or want to use some special features, please submit an issue or a pull request on [openeuler-docker-images](https://gitee.com/openeuler/openeuler-docker-images).